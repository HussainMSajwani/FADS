\contentsline {figure}{\numberline {1}{\ignorespaces A graphical representation of a neural network. Each colored box represents a layer. The connections (black arrows) between the layers are the weights $W^{(l)}.$ This image was downloaded from: \url {https://cs231n.github.io/neural-networks-1/.}}}{5}{figure.1}%
\contentsline {figure}{\numberline {2}{\ignorespaces An example of an autoencoder with two layers in the encoder and decoder parts. The red units are the bottleneck. This image was downloaded from \url {https://www.compthree.com/blog/autoencoder/}}}{5}{figure.2}%
\contentsline {figure}{\numberline {3}{\ignorespaces Demonstration of ROC curves. The leftmost panel represents the perfect scenario for a classifier in which it has learnt a perfect separation to the observation based on their class. Given an observation, a classifier with AUC=100\% is able to always predict which class the observation belongs to. The second panel shows a more attainable ROC curve. The third panel shows a case where the classifier has not learnt anything and leaves classification up to chance. The last panel shows an ROC curve of a model that does worse than chance. It will always give a wrong label. The picture was downloaded from \url {https://www.datasciencecentral.com/profiles/blogs/roc-curve-explained-in-one-picture}}}{8}{figure.3}%
\contentsline {figure}{\numberline {4}{\ignorespaces \emph {Left:} Each panel has a histogram of the original genotype matrix $X$ and a histogram of $\hat {X}$, the autoencoder's attempt at reconstructing $X.$ The genotype is simulated with parameters $(600, 1000, 10, 0.5)$. The autoencoder is two layers deep with $\rho =0.8$ with $k \in \{5, 10, 20, 50, 100, 250, 500, 750, 1000\}$ \emph {Right: }Wasserstein distance from original histogram to reconstructed ones from autoencoders with size $k$.}}{9}{figure.4}%
\contentsline {figure}{\numberline {5}{\ignorespaces Correlation matrices of extracted features of ReLU activated SAE. Each panel represents the results of a SAE trained with $\rho =0.2, 0.6, $ and $1$, respectively. Each panel contains nine subpanels which show the correlation matrices for values of $k \in \{5, 10, 20, 50, 100, 250, 500, 750, 1000\}.$ The leftmost, $\rho =1$, panel confirms theory that an AE will learn a diagonal correlation matrix, like PCA. The other two SAE with $\rho \neq 1$ do not learn uncorrelated features. Note that if an entry on the diagonal is zero, then the SAE learnt a constant feature.}}{10}{figure.5}%
\contentsline {figure}{\numberline {6}{\ignorespaces Violin plots of precision and recall of both thresholding methods over all 25 datasets for each value of $h_2^s$ and $k$. In both precision plots, precision goes up as $h_2^s$ increases and goes down as $k$ increases. Recall increase with both $k$ and $h_2^s$. \emph {Top left}: precision of $p$ value thresholding. In all circumstances, $p$ thresholding has zero variance. \emph {Top right:} recall of $p$ thresholding. This also has zero variance. \emph {Bottom left}: precision of AE weights thresholding. High variance for lower values of $k$ and decreases with $h_2^s.$ \emph {Bottom right:} Recall of AE thresholding. Low variance on lower values of $k.$ }}{11}{figure.6}%
\contentsline {figure}{\numberline {7}{\ignorespaces Example thresholding on a single dataset with $h_2^s = 0.75$ and both thresholding methods. }}{12}{figure.7}%
\contentsline {figure}{\numberline {8}{\ignorespaces Classification results of three models trained after reducing dimensionality using PCA. In all three models, PCA has done a bad job of extracting information that is useful for classification. The AUC barely goes above 0.8 in some cases. }}{12}{figure.8}%
\contentsline {figure}{\numberline {9}{\ignorespaces Classification results of three models trained after reducing dimensionality using SAE with $\rho =0.3$. All methods seem to be doing better than PCA. \emph {Top left:} the performance of the supervised part of the supervised autoencoder. \emph {Top right:} results of an SVM trained on the latent space of the autoencoder}}{14}{figure.9}%
\contentsline {figure}{\numberline {10}{\ignorespaces Classification results of three models trained after reducing dimensionality using thresholding.}}{15}{figure.10}%
