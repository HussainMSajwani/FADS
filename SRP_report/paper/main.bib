@book{wainright,
author = {Wainwright, Martin},
year = {2019},
month = {02},
pages = {},
title = {High-Dimensional Statistics: A Non-Asymptotic Viewpoint},
isbn = {9781108498029},
doi = {10.1017/9781108627771},
publisher={Cambridge University Press
}
}
@article{PS,
    author = {Meyer, Hannah Verena and Birney, Ewan},
    title = "{PhenotypeSimulator: A comprehensive framework for simulating multi-trait, multi-locus genotype to phenotype relationships}",
    journal = {Bioinformatics},
    volume = {34},
    number = {17},
    pages = {2951-2956},
    year = {2018},
    month = {03},
    abstract = "{Simulation is a critical part of method development and assessment. With the increasing sophistication of multi-trait and multi-locus genetic analysis techniques, it is important that the community has flexible simulation tools to challenge and explore the properties of these methods.We have developed PhenotypeSimulator, a comprehensive phenotype simulation scheme that can model multiple traits with multiple underlying genetic loci as well as complex covariate and observational noise structure. This package has been designed to work with many common genetic tools both for input and output. We describe the underlying components of this simulation tool and illustrate its use on an example dataset.PhenotypeSimulator is available as a well documented R/CRAN package and the code is available on github: https://github.com/HannahVMeyer/PhenotypeSimulator.Supplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/bty197},
    url = {https://doi.org/10.1093/bioinformatics/bty197},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/34/17/2951/25703101/bty197.pdf},
}
@article{montanez,
  author    = {Casimiro Aday Curbelo Monta{\~{n}}ez and
               Paul Fergus and
               Almudena Curbelo Monta{\~{n}}ez and
               Abir Hussain and
               Dhiya Al{-}Jumeily and
               Carl Chalmers},
  title     = {Deep Learning Classification of Polygenic Obesity using Genome Wide
               Association Study SNPs},
  journal   = {CoRR},
  volume    = {abs/1804.03198},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.03198},
  archivePrefix = {arXiv},
  eprint    = {1804.03198},
  timestamp = {Mon, 13 Aug 2018 16:48:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-03198.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{alsnet,
    author = {Yin, Bojian and Balvert, Marleen and van der Spek, Rick A A and Dutilh, Bas E and Bohté, Sander and Veldink, Jan and Schönhuth, Alexander},
    title = "{Using the structure of genome data in the design of deep neural networks for predicting amyotrophic lateral sclerosis from genotype}",
    journal = {Bioinformatics},
    volume = {35},
    number = {14},
    pages = {i538-i547},
    year = {2019},
    month = {07},
    abstract = "{Amyotrophic lateral sclerosis (ALS) is a neurodegenerative disease caused by aberrations in the genome. While several disease-causing variants have been identified, a major part of heritability remains unexplained. ALS is believed to have a complex genetic basis where non-additive combinations of variants constitute disease, which cannot be picked up using the linear models employed in classical genotype–phenotype association studies. Deep learning on the other hand is highly promising for identifying such complex relations. We therefore developed a deep-learning based approach for the classification of ALS patients versus healthy individuals from the Dutch cohort of the Project MinE dataset. Based on recent insight that regulatory regions harbor the majority of disease-associated variants, we employ a two-step approach: first promoter regions that are likely associated to ALS are identified, and second individuals are classified based on their genotype in the selected genomic regions. Both steps employ a deep convolutional neural network. The network architecture accounts for the structure of genome data by applying convolution only to parts of the data where this makes sense from a genomics perspective.Our approach identifies potentially ALS-associated promoter regions, and generally outperforms other classification methods. Test results support the hypothesis that non-additive combinations of variants contribute to ALS. Architectures and protocols developed are tailored toward processing population-scale, whole-genome data. We consider this a relevant first step toward deep learning assisted genotype–phenotype association in whole genome-sized data.Our code will be available on Github, together with a synthetic dataset (https://github.com/byin-cwi/ALS-Deeplearning). The data used in this study is available to bona-fide researchers upon request.Supplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btz369},
    url = {https://doi.org/10.1093/bioinformatics/btz369},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/35/14/i538/28913765/btz369.pdf},
}

@inproceedings{liu,
author = {Liu, Bo and Wei, Ying and Zhang, Yu and Yang, Qiang},
title = {Deep Neural Networks for High Dimension, Low Sample Size Data},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Deep neural networks (DNN) have achieved breakthroughs in applications with large sample size. However, when facing high dimension, low sample size (HDLSS) data, such as the phenotype prediction problem using genetic data in bioinformatics, DNN suffers from overfitting and high-variance gradients. In this paper, we propose a DNN model tailored for the HDLSS data, named Deep Neural Pursuit (DNP). DNP selects a subset of high dimensional features for the alleviation of overfitting and takes the average over multiple dropouts to calculate gradients with low variance. As the first DNN method applied on the HDLSS data, DNP enjoys the advantages of the high nonlinearity, the robustness to high dimensionality, the capability of learning from a small number of samples, the stability in feature selection, and the end-to-end training. We demonstrate these advantages of DNP via empirical results on both synthetic and real-world biological datasets.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2287–2293},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@book{dlbook,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{tied,
author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
title = {Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {3371–3408},
numpages = {38}
}

@book{islr,
	Author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	Booktitle = {An Introduction to Statistical Learning},
	Date-Modified = {2015-07-26 12:59:11 pm +0000},
	Doi = {10.1007/978-1-4614-7138-7},
	Isbn = {978-1-4614-7137-0},
	Language = {English},
	Publisher = {Springer New York},
	Series = {Springer Texts in Statistics},
	Title = {An Introduction to Statistical Learning: with Applications in R},
	Url = {http://doi.org/10.1007/978-1-4614-7138-7},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-1-4614-7138-7_5}
    }
    
@article{aepca,
title = {Neural networks and principal component analysis: Learning from examples without local minima},
journal = {Neural Networks},
volume = {2},
number = {1},
pages = {53-58},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90014-2},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900142},
author = {Pierre Baldi and Kurt Hornik},
keywords = {Neural networks, Principal component analysis, Learning, Back propagation},
abstract = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed.}
}

@article{wasserstein,
 ISSN = {00029939, 10886826},
 URL = {http://www.jstor.org/stable/20535091},
 abstract = {We give an elementary proof for the triangle inequality of the p-Wasserstein metric for probability measures on separable metric spaces. Unlike known approaches, our proof does not rely on the disintegration theorem in its full generality; therefore the additional assumption that the underlying space is Radon can be omitted. We also supply a proof, not depending on disintegration, that the Wasserstein metric is complete on Polish spaces.},
 author = {Philippe Clement and Wolfgang Desch},
 journal = {Proceedings of the American Mathematical Society},
 number = {1},
 pages = {333--339},
 publisher = {American Mathematical Society},
 title = {An Elementary Proof of the Triangle Inequality for the Wasserstein Metric},
 volume = {136},
 year = {2008}
}

@misc{adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{sae,
 author = {Le, Lei and Patterson, Andrew and White, Martha},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Supervised autoencoders: Improving generalization performance with unsupervised regularizers},
 url = {https://proceedings.neurips.cc/paper/2018/file/2a38a4a9316c49e5a833517c45d31070-Paper.pdf},
 volume = {31},
 year = {2018}
}

@misc{tf,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{dropout,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1929–1958},
numpages = {30},
keywords = {deep learning, model combination, regularization, neural networks}
}

@article{bonferroni,
author = {Goeman, Jelle J. and Solari, Aldo},
title = {Multiple hypothesis testing in genomics},
journal = {Statistics in Medicine},
volume = {33},
number = {11},
pages = {1946-1978},
keywords = {FDR, false discovery rate, false discovery proportion, familywise error rate, Bonferroni},
doi = {https://doi.org/10.1002/sim.6082},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6082},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6082},
abstract = {This paper presents an overview of the current state of the art in multiple testing in genomics data from a user's perspective. We describe methods for familywise error control, false discovery rate control and false discovery proportion estimation and confidence, both conceptually and practically, and explain when to use which type of error rate. We elaborate on the assumptions underlying the methods and discuss pitfalls in the interpretation of results. In our discussion, we take into account the exploratory nature of genomics experiments, looking at selection of genes before or after testing, and at the role of validation experiments. Copyright © 2014 John Wiley \& Sons, Ltd.},
year = {2014}
}


