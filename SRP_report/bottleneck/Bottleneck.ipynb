{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "treated-problem",
   "metadata": {},
   "source": [
    "(I am envisioning that there will be a section above this explaining the gist of what we are trying to do and the notation (what k is, p-thresholding, and so on))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-exchange",
   "metadata": {},
   "source": [
    "# Size of bottleneck\n",
    "When training our first autoencoder on our training data, we used our a priori knowledge of the number of causal SNPs to choose k, the dimension of the latent representation of our data. In this particular case we have 10 causal SNPs so we set our autoencoder to have k=10. The autoencoder consists of three layers: the input (encoder), the latent representation, and the output (decoder). After training the autoencoder one would expect the autoencoder to have learnt the underlying structure (distribution) of our dataset. \n",
    "\n",
    "\n",
    "![Original Histogram](og.jpeg)\n",
    "\n",
    "Consider the figure above depicting the histogram of _all_ of our dataset $X$. We see that the majority of our dataset does not contain our reference allele. As previously stated we expect that, after passing the training data through the trained autoencoder, to have a similar histogram. However, as can be seen from the below figure this did not happen. It seems that the autoencoder learnt to completely forget about homozygous pairs with two of the reference allele. \n",
    "\n",
    "![Reconstructed Histogram](out_k20.jpeg)\n",
    "\n",
    "This is not wanted behavior as we want to keep the twos. We came up with two reasons for why that might happen: 1) the model was not trained enough to take into account the twos as they are underrepresented in the dataset. 2) k was too small. To test the first hypothesis we trained the model on a modified dataset $X_u$ where each $x_{ij} \\mathcal{U}\\{0, 1, 2\\}.$ The results of training can be seen below\n",
    "\n",
    "![Unif Histogram](out_unif_k20.jpeg)\n",
    "\n",
    "The trend of the autoencoder focusing all of its power on a specific region of the range persists here. We can see that the autoencoder learnt to predict 1 as much as possible and discard the other values. (the peak at zero can probably explained by the fact that the model is ReLU activated.) Now we turned our attention to k. In the following expriment we train nine autoencoders with different values of k and plot the histograms of the output.\n",
    "\n",
    "![expriment](res.jpeg)\n",
    "\n",
    "This expriement confirms that we were bottlenecking the autoencoder too hard. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
